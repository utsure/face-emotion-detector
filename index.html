<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <title>AI感情認識カメラ (face-api.js)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
  <style>
    body { font-family: 'Arial', sans-serif; margin: 0; background: #2c3e50; color: #ecf0f1; display: flex; flex-direction: column; align-items: center; justify-content: flex-start; min-height: 100vh; overflow: hidden; padding-bottom: 20px; }
    #info-text { font-size: 32px; font-weight: bold; padding: 20px 10px 10px; text-align: center; width: 100%; box-sizing: border-box; color: #f39c12; }
    #emoji-display { font-size: 90px; text-align: center; width: 100%; padding: 0 10px 20px; line-height: 1; }
    #video-container { position: relative; width: 95vw; max-width: 600px; margin: 0 auto; border-radius: 15px; overflow: hidden; box-shadow: 0 8px 16px rgba(0, 0, 0, 0.4); background-color: #34495e; }
    #video-container::before { content: ''; display: block; padding-top: calc(3 / 4 * 100%); }
    video, canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); }
    canvas { z-index: 10; }
  </style>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
</head>

<body>
  <div id="info-text">AIモデルを準備中...</div>
  <div id="emoji-display">⏳</div>
  <div id="video-container">
    <video id="input_video" autoplay muted playsinline></video>
    <canvas id="output_canvas"></canvas>
  </div>

  <script>
    const video = document.getElementById('input_video');
    const infoText = document.getElementById('info-text');
    const emojiDisplay = document.getElementById('emoji-display');

    const emotionEmojis = {
      "neutral": "😐", "happy": "😊", "sad": "😔", "angry": "😠", "surprised": "😮", "fearful": "😨", "disgusted": "🤢", "No Face": "🤔"
    };

    // AIモデルの読み込み
    async function loadModels() {
      // face-api.jsに必要なモデルファイルを 'models' フォルダから読み込む
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri('./models'),
        faceapi.nets.faceLandmark68Net.loadFromUri('./models'),
        faceapi.nets.faceRecognitionNet.loadFromUri('./models'), // これは通常顔認識用ですが、念のため
        faceapi.nets.faceExpressionNet.loadFromUri('./models')  // ★感情認識のメインモデル
      ]);
      infoText.innerText = "カメラにアクセス中...";
      emojiDisplay.innerText = "📷";
      startVideo();
    }

    // カメラの起動
    function startVideo() {
      navigator.mediaDevices.getUserMedia({ video: {} })
        .then(stream => {
          video.srcObject = stream;
        })
        .catch(err => {
          console.error("カメラエラー: ", err);
          infoText.innerText = "エラー: カメラ起動に失敗";
          emojiDisplay.innerText = "🚫";
        });
    }

    // ビデオが再生開始されたら、感情認識ループを開始
    video.addEventListener('play', () => {
      const canvas = faceapi.createCanvasFromMedia(video);
      document.getElementById('video-container').append(canvas);
      const displaySize = { width: video.clientWidth, height: video.clientHeight };
      faceapi.matchDimensions(canvas, displaySize);

      infoText.innerText = "顔を検出中...";
      emojiDisplay.innerText = "🤔";

      setInterval(async () => {
        const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();
        
        // 描画をクリア
        canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
        
        if (detections && detections.length > 0) {
          const resizedDetections = faceapi.resizeResults(detections, displaySize);
          
          // 顔の周りのボックスとランドマークを描画 (確認用)
          faceapi.draw.drawDetections(canvas, resizedDetections);
          faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
          
          // 感情を取得して表示
          let highestEmotion = "neutral";
          let highestValue = 0;
          const expressions = resizedDetections[0].expressions;

          for (const emotion in expressions) {
            if (expressions[emotion] > highestValue) {
              highestValue = expressions[emotion];
              highestEmotion = emotion;
            }
          }
          
          infoText.innerText = `Emotion: ${highestEmotion}`;
          emojiDisplay.innerText = emotionEmojis[highestEmotion];

        } else {
          infoText.innerText = "顔を検出中...";
          emojiDisplay.innerText = "🤔";
        }
      }, 100); // 100ミリ秒ごとに認識
    });

    // 最初にモデルを読み込む
    loadModels();
  </script>
</body>
</html>
